<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<link rel="icon" type="image/png" href="https://lh3.googleusercontent.com/0NI3S_45B7BNc5XDpKQSTdTjZdD-wjjhJ0XTg-fMy8PXI52lkJEKYwQhEBlDh5RgLC_AvPjVq_li7zPd2zHqX0beZEurbvYdFlpodAuMjHao1rYXzRPG4Rzx7T_9SmsxOVrEC28QE9cXYoYWmaUODs2sMr1ZSKZAPnD-T2zoNNtjxwri0iPHQn70W6jp4CYztbyXn4s9wrqhjLy3TGe-hTe8h-Od999xYeWO1gjtHHpSZz15ToNpi0-ECriNX31NS9Ff0uWXiW3FiLmlo9cLD3rcv97tHmLCJVhqeLOSO6hHwajxH-EEQFNdMmKABr00og6tVVuhvN_GZ3dHXmlMcNitem_PTggXgX4jgMu1vEMDnoKhB0cywUWvlwgnBkV2FRgXW2i2eEBgt9UUve4HO_XJkbYdd9533CQ5GkOGebp2XpWY0iKspLRoQ-z_hCZmZOnSN8wm8mKajD-zCYN6bAzJtg8Hg6IO7bAhrvkCCOJQjlpokH-2M_fxhQ10oGW4dF1Sp7cIpCkneElT-zLB8uTknAlCGmOfgMCTU-o0vHtHzH1Dbk2XaMQD60e9YpJBihx_fD50OQmef3Iw_VOvO0YSBPwvuRr62GLHtcB0GkLcEEe-etlU98neULIJtDwTHueHeWhZymQMaur2nI_8nc9xDSOq3WFu=s100-no"> 
	<title>Thanh Vu</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
</head>

<body>

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Main -->
		<div id="main">
			<div class="inner">

				<!-- Header -->
				<header id="header" class="thanh-header-bar">
					<a href="" class="logo"><strong>Welcome!</strong></a>
					<!-- <ul class="actions">
						<li><a href="#cv" class="button small">cv</a></li>
						<li><a href="#research" class="button small">research</a></li>
						<li><a href="#teaching" class="button small">teaching</a></li>
					</ul> -->

					<ul class="icons">
						<!-- <li><a href="#" style="font-weight: bold;"><span class="label">CV</span></a></li> -->
						<li><a href="##resume##" class="icon fa-file-text-o" style="color: #468EC9 !important;"><span class="label">Resume</span></a></li>
						<li><a href="mailto:tvu-[at]-cs-[dot]-unc-[dot]-edu" class="icon fa-envelope-o" style="color: #468EC9 !important;"><span class="label">Email</span></a></li>
						<li><a href="https://www.linkedin.com/in/thanhmvu/" class="icon fa-linkedin" style="color: #468EC9 !important;"><span class="label">Linkedin</span></a></li>
						<li><a href="https://github.com/thanhmvu" class="icon fa-github" style="color: #468EC9 !important;"><span class="label">Github</span></a></li>
					</ul>
				</header>
				<br/>

				<!-- Intro -->
				<section id="banner" class="thanh-banner">
					<div class="content">
						<header>
							<h1>Thanh M. Vu</h1>
							<h2><p>Phd Student, UNC Chapel Hill</p></h2>
							<!-- <p>
								tvu [at] cs [dot] unc [dot] edu <br>
								<a href="http://www.cs.unc.edu/~tvu/">http://www.cs.unc.edu/~tvu</a>
							</p> -->
						</header>
						
						<h3 style="font-weight: normal;">
							<p>I am a PhD student at <a href="http://cs.unc.edu/">UNC Chapel Hill</a>, advised by Dr. <a href="http://frahm.web.unc.edu/">Jan-Michael Frahm</a>. My  research interests lie in the areas of Computer Vision and Machine Learning. Previously, I received my BS in Computer Science and a Minor in Mathematics from <a href="https://www.lafayette.edu/">Lafayette College</a>, Easton PA, where I had the chance to work with Dr. <a href="https://scholar.google.com/citations?user=OBuu9_YAAAAJ&hl=en">Amir Sadovnik</a> and Dr. <a href="https://compsci.lafayette.edu/people/chun-wai-liew/">Chun Wai Liew</a> on multiple research projects.</p>
			    		</h3>

						<!-- <p><font size="5">I am an incoming PhD student at the University of North Carolina at Chapel Hill. I previously completed my Bachelor's in Computer Science at Lafayette College, with a Minor in Mathematics. My primary research interest lies in the areas of computer vision and machine learning.</font></p> -->
						<ul class="actions">
							<!-- <li><a href="#" class="button special small">CV</a></li> -->
							<li><a href="##resume##" class="button special small">Resume</a></li>
							<li><a href="/#research" class="button special small">research</a></li>
							<li><a href="/#employment" class="button special small">employment</a></li>
							<li><a href="/art" class="button special small">photography</a></li>
							<!-- <li><a href="dev" class="button special small">coding</a></li> -->
						</ul>
					</div>
					<span class="image object">
						<img src="https://lh3.googleusercontent.com/wzNM4FMVcqk8RiPvc6Z3atByp15pdI3C9no039xDufLyzGEZ9QBG5RVLUqrVpuHrb6Pu8dP2b9gReFh_i2s5fKo6qnq2tDa3uH4K39lH3BZO5YXnDdaDPnXzavhUdW8EgKvlez2KAQL5PfyZDlAFvV3yrbRk7RBxF2EyA2QTFwd5t_gsRCJt4deqh65fBv6JsGigZofDKYqVRdOTnxKdFh1v8uUSOFJ9bIoO9oT7EXXzc4vw6s1l-2pBL8oPlslqvO6Rn1Lanrto1wyljpnsIrCoDT1y061KbkbghDTCGUbeYvOBirrXOgOqvIx2Ok7Q-SNrb45idryE6IObmizW_YESCHHCsBOyICZ_4O6Or2ZNAyjaK8OcFbQpyJEevrRP896sdQ5CSpiPvtmz6FPUUeyXi0zfabey8lyopAz4huI16vsLj0-orY-GidBQS4xj22NJ_6iiYRV4setEnDNBs1YlnxBtfQFs6qsv27tBDXzOLCje78EDQ8m2OhCq4oMTy5R--k55CboURD-Xk2n0DfnPMnBpcCTIY8B3Dl7zhFWmclQuYijK-w44se7yf6NMZ6SeqKTU-iH-wWmm285QDeZX7JAttruLGt-tg239loDZOKh6-57_PZuZa7EMO7AgyqjfBIYsI_sU4tAoRTpV9gZqTiTnDkvl=w757-h758-no" alt="" />
					</span>
				</section>

				<header id="header" class="thanh-footer-bar"></header>

				<!-- Research -->
				<section id="research">
					<header class="major">
						<h2>RESEARCH</h2>
					</header>
					
					<div class="t-posts">
						<t-article-image id="faceSim" class="t-research-img">
							<a href="#faceSim" class="image"><img src="https://lh3.googleusercontent.com/GE2SOGdHkACagBf15Vsq6sXuCH3rgs4zZcGjtdBUJ95PNPEUwKHUBIX3nvblB2GE0vz_1EQjzP0Wyh9USd50afhpCyUTAx57MB7-wogXB9e9p8PY32Ke_-5zDSDWR6TaziQroEJ-1rKcpb4lLhKUxP4vlNMcVj3juRYa9sXAhMFKvaLRLEmazu5vwCupnvG4gkhi5N80K4JQjBdn3ElG0RN8ivY-LnVMuNl_9u0wUugt4a31UFvGGuLI89-4TuD-zKAcnsBBEh5_hK6xs6Fr8Ck5RujuytFtR7-aRf0GnqYS9PhpWzZq8Dz8VwUfLJqR8kv7clG7hvzzc7H5AwMHsu9BduKSnZFmt2Ht4xYE3HX13W1ZOCxWWYMKsvUZLGUpCKzYOQmkGDzIyZl8ymA50zPHXw05n2WYiBRVVPImHBpmVQ5EuQWiZCe1nIz5QhO3dLooGyjr4a8RYz_DEVy5iG2iqKIXncQIgb9fnauJRZLJ02ES7Gk29qBnhu09FfULHju3zXgR7DBumcwVI8aCEkcI47StOQHQFUGOnuEf_lt-i-YgHScuJTq12x0JhyWerGydqt2ZUBFnfuiXTnhRb8kWZWkj_x3QDdhHTYX72tny7u4_U_fE_polaSB3m-23Q9P1gLGS4ui8giEgrLvQ3vap_LsiapD1=s337-no" alt="" /></a>
						</t-article-image>
						<t-article-text id="faceSim-description" class="t-research-text">
							<h2><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w48/html/Sadovnik_Finding_Your_Lookalike_CVPR_2018_paper.html" style="border-bottom: 0px;">Finding your Lookalike: Measuring Face Similarity Rather than Face Identity</a></h2>
							<h3 style="font-weight: normal">
								<a href="https://scholar.google.com/citations?user=OBuu9_YAAAAJ&hl=en">Amir Sadovnik</a>,
								<a href="http://gharbi.me/">Wassim Gharbi</a>,
								<strong>Thanh Vu</strong>,
								<a href="https://ai.google/research/people/AndrewGallagher">Andrew Gallagher</a>
							</h3>
							<h3 style="font-weight: normal">
								Computer Vision and Pattern Recognition (CVPR) Workshops, 2018 
							</h3>
							<!-- <br> -->
							<!-- <p>Face images are one of the main areas of focus for computer vision, receiving on a wide variety of tasks. Although face recognition is probably the most widely researched, many other tasks such as kinship detection, facial expression classification and facial aging have been examined. In this work we propose the new, subjective task of quantifying perceived face similarity between a pair of faces. That is, we predict the perceived similarity between facial images, given that they are not of the same person. Although this task is clearly correlated with face recognition, it is different and therefore justifies a separate investigation. Humans often remark that two persons look alike, even in cases where the persons are not actually confused with one another. In addition, because face similarity is different than traditional image similarity, there are challenges in data collection and labeling, and dealing with diverging subjective opinions between human labelers. We present evidence that finding facial look-alikes and recognizing faces are two distinct tasks. We propose a new dataset for facial similarity and introduce the Lookalike network, directed towards similar face classification, which outperforms the ad hoc usage of a face recognition network directed at the same task.</p> -->
							<p>In this work, we propose a new, subjective task of quantifying perceived face similarity between a pair of faces: Predicting similarity between facial images that are not of the same person. We present evidence that finding facial look-alikes and recognizing faces are two distinct tasks. Moreover, we propose a new dataset for facial similarity and introduce the Lookalike network, directed towards similar face classification, which outperforms the ad hoc usage of a face recognition network for the same task.</p>
							<ul class="actions">
								<li><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w48/html/Sadovnik_Finding_Your_Lookalike_CVPR_2018_paper.html" class="button small">pdf</a></li>
								<li><a href="./refs/FaceSim_Sadovnik_2018_CVPR_Workshops.bib" class="button small">bibtex</a></li>
							</ul>
						</t-article-text>
					</div>
					
					<header id="header" class="thanh-horizontal-separator"></header>
					
					<div class="t-posts">
						<t-article-image id="thesis" class="t-research-img">
							<a href="#thesis" class="image"><img src="https://lh3.googleusercontent.com/rowbgbisvM_o8bCqVW7dVwLWjLZ45ASe9Cwt1Z9W47HfLjwImpsHQj_CVbpI10qcbP2qiY7pVQZQy4oiFPo9hlDIXinyUp1XQIDTsM1S8gZaIIG9sMgkKliYT-nL7dtSTOZp483rnvY4FOzYHeAPgfK6TfiMl_IyMoHkWBoCllomDFrb1cqD07GPeRoP2MbTyRBBGrgV0DdKDNjPUhVCyBL3GpBVRtRCO2EjlhgJsqSzAicXAwp6ZtGMwGLrZ5T6KnOvMs1n4hWBjkQyfAv71vV5fm98M1Km2eTcxwHBJ4rvXHMwZozruSN8FgVJHptJvxFvOScDyrv_CzYZWd4qCrcs8PYxFe5bEcnnwz0Kak1j-c-yK8OBOeWmcuhCLq6qbXUDZIwkKi4DhvSvBbaI1oCACPZjIxu0bJoIJFN1OKNgU7LEnQZdSSRnci0-B8TOO586oPsYi3AK6YmC4MDbTBDladIF6lszLLdtQ3vDLckGWcheNMFfhpILAxFq3SldYufM7vAMhgBkjoldn0UCq6FvrXCLFDsH9BW1cwiEBXDeUaD5cjVRai3JfP5y2cU5IJIsriXkOBRojJw1g2mOmjVgJfZKmYD8ExjHxjJMrCna_nvHjqoyb9FyK51PEbsAGQNajXYvHxBHlsvvslTyFBXvAVcPoPJZ=s1259-no" alt="" /></a>
						</t-article-image>
						<t-article-text id="thesis-description" class="t-research-text">
							<h2><a href="https://drive.google.com/open?id=1sDmlBC-252EU31fabgctF5ClIA-P-rVo" style="border-bottom: 0px;">Learning Visual Compatibility: An Improved Method for Visual Compatibility Embedding</a></h2>
							<h3 style="font-weight: normal">
								<strong>Thanh Vu</strong>
							</h3>
							<h3 style="font-weight: normal">
								An honors thesis for BS in Computer Science, Lafayette College, 2018
							</h3>
							<!-- <br> -->
							<!-- <p>Visual compatibility is a visual property that allows cross-category objects to coordinate well with each other, creating a visually cohesive and appealing collection. This work studies the problem of compatibility learning using deep convolutional neural networks, focusing on the fashion domain. Particularly, it explores a metric learning method to embed images into a compatibility space where more compatible objects are closer to each other. We propose a notion of a visual compatibility hierarchy with three different levels: functional, contextual, and aesthetic compatibility. Based on the levels’ increasing complexities, we propose a training framework that learns the property in a ranking manner and a data-driven method for learning such a hierarchy. Our experimental results demonstrate that the framework outperforms previously proposed methods for embedding visual compatibility. Moreover, our method can learn the property hierarchically and classify aesthetic compatibility from contextual compatibility, which is a much harder task compared to compatible/incompatible classification and is task that has not been addressed directly before.</p> -->
							<p>Visual compatibility is a property that allows objects of different types to coordinate well and create a visually cohesive and appealing collection. This work studies a deep metric learning method for compatibility learning, focusing on the fashion domain. We propose a notion of visual compatibility hierarchy, a training framework that learns in a ranking manner, and a data-driven method for learning such a hierarchy. Our method outperforms previously proposed approaches and enables our CNN to perform aesthetic/contextual compatibility classification, a challenging but unexplored task.</p>
							<ul class="actions">
								<li><a href="https://drive.google.com/open?id=1sDmlBC-252EU31fabgctF5ClIA-P-rVo" class="button small">pdf</a></li>
								<li><a href="./refs/VisualComp_Vu_2018_Lafayette.bib" class="button small">bibtex</a></li>
							</ul>
						</t-article-text>
					</div>
					
					<header id="header" class="thanh-horizontal-separator"></header>
					
					<div class="t-posts">
						<t-article-image id="rapr">
							<a href="https://github.com/thanhmvu/PosterRecognition" class="image"><img src="https://lh3.googleusercontent.com/_ynAGoaXpxCPOMoxCnG_6baBIwzWUn4UgR4zf-LZg8HvtjOXlmMrERqNrThSCxzD3TMaLH3rfwAznDDrIEdv7AIxQ1-UKhCCD1pTdBwBCtfyts0p4YMv6nEcwG00cL2E9VsqD6jGXXswR_7Zwsq6UKp9Yt-KKA9q8hphLfK1jvnrE0QXOOMkw4McMOgK3jngkjKrdMilbUrM410eiPR_fgDKaSbQzs-l9BnOu6TSqoAzshOniCZzlXuhHTmYaQ3IpiZzzXXoqb4-mk12BecNbPo0Xl8_Ie4JALSyfVKX2HjAWJaZ8_5F5Um1e_cKqYYAojng26W3m60svfb9lNxksH5mXVzTllD1LLTMOEHmcTOxWL2q5xPY76eYbeSbseOFphgFIliyhrUTY2ByINbrF_KGZue3oAGeXRVOSQG-_pRMU6X3qozaJhSTns1X90QU1tREVmrknjwh1S4UquVbJpNtH2sQmYE6Zxe9iR49q-CaTdVnD1DQ8Zj66GSv3XviIpvZg_PbEywQW00uk18GNz4W7J4YUjVy00nRgyZFdczFpDJyZa8iVPXFf28_iHJVT75C3pD3Fl7BbQSVaY205qipYs3d4poWvIgGDqefYdDepFoxjZJ-teVhlmo5m2GH5vTiWTULKBNZcI03VdD7z_vQyE8eTyXo=s577-no" alt="" /></a>
						</t-article-image>
						<t-article-text id="rapr-description" class="t-research-text">
							<h2><a href="https://drive.google.com/file/d/0B2TC6ATRSctxci1uTlFjNU1sR1k" style="border-bottom: 0px;">Robust Academic Poster Recognition</a></h2>
							<h3 style="font-weight: normal">
								<strong>Thanh Vu</strong>,
              					<a href="https://scholar.google.com/citations?user=OBuu9_YAAAAJ&hl=en">Amir Sadovnik</a>
              				</h3>
							<h3 style="font-weight: normal">
								A research completed under EXCEL Scholars Program, Lafayette College, 2017
							</h3>
							<!-- <br> -->
							<p>Traditional image recognition is done by comparing features extracted from the query image to features extracted from an image database. Although producing state of the art results for many types of objects, this approach does not generalize to all domains. In this paper we address one such domain: academic poster recognition. First we show that because of the unique structure of academic posters and the environment in which they are presented, the traditional method of feature matching fails. Then we present a new approach for academic poster recognition based on object detection using convolutional networks and show how it outperforms the traditional approaches.</p>
							<!-- <p>This work addresses a challenging but unexplored problem of image recognition: Academic poster recognition in conferences. We overcame the problems of posters' high text density and occlusions using an object detection CNN. Deviating from the traditional data-collecting process in deep learning, we instead automatically generated thousands of realistic synthetic training images. The network trained with our method outperformed traditional approaches with an accuracy of 92.8%.</p> -->
							<ul class="actions">
								<li><a href="https://drive.google.com/file/d/0B2TC6ATRSctxci1uTlFjNU1sR1k" class="button small">pdf</a></li>
								<li><a href="./refs/PosterRecognition_Vu_2017_Lafayette.bib" class="button small">bibtex</a></li>
								<li><a href="https://drive.google.com/open?id=0B4dOg-7qH-2jNlhqT0RUbzlpVW8" class="button small">poster</a></li>
								<li><a href="https://github.com/thanhmvu/PosterRecognition" class="button small">code</a></li>
								<li><a href="https://photos.google.com/share/AF1QipNKkFxrgbcIN0NTb1IkVjMQPUx3_OPOXTwHkZesKlinWpJmLTXcLW6e-PLhlZm_yg?key=MS1PbV9WcVFQZ0ZsNmpnLWdWNzNFaFlTamRneDZn" class="button small">data</a></li>
							</ul>
						</t-article-text>
					</div>
					
					<header id="header" class="thanh-horizontal-separator"></header>
					
					<div class="t-posts">
						<t-article-image id="ambr">
							<a href="http://www.ncurproceedings.org/ojs/index.php/NCUR2016/article/view/1969" class="image"><img src="https://lh3.googleusercontent.com/eLkFAWV7GZ2YQa_2wrytHRVexV-RnJYHH_n0RdQyjOGJ7p2kQJBrz93aY74yjmx-I67agt7oZzWHlHBczXCt5VfQfoLmZyO0g9Alnc1SRU6w4J-IJDtNPmQCwZFqNHK5ss9R_MWyA8BWMBy5JvENR9Ea0Wm_X1AilGfAchFbtMzd5tU7Jm_7IDhGIyfY83RXxyJZ-9Rm4S17scZILff1hbOFr0WeJfjBPZ9dJFm7vyNy5K1vxJgFWSC8ruoRk2u2ETrUS0itH2SxGy0AQ0jTyvPVoTLSzXI0Z3cbvAzuziRq_LoetNmqsLhP-FAo8wnlHrmXp_KwClJ_yXE6cW0DziYKcTnxNWLnbjLRaQj9YB-tpt4C6h4UsnxnQPoT2gAr-8BR3DuD6mLlEVlBqJPqu2o2ZBHwuCtM_zYskmQiqySL0xp7fHylhhgeqlaLsFC__o-O_tMQeS26qVw7yuPwf_6pHCC5vb1PHMm5uaNBR3PxrgqA2CjSOKhAGgV7F78pqUdKs2s4zyYLoVY7Gb79HmqTdIpFaWxi-olokTS0S2ELYcrZqzSIKUfXj4ORsDB-WPK2G6JWf8Q6hviC1BT5hbtO0MXAwfe7gXULJLyjoKc3qENYfDO_O8edxCZ6DdWZXQmbrGRUzSASC-c6dZUBGxxdVQnbC30k=w1062-h1113-no" alt="" /></a>
						</t-article-image>
						<t-article-text id="ambr-description">
							<h2><a href="http://www.ncurproceedings.org/ojs/index.php/NCUR2016/article/view/1969" style="border-bottom: 0px;">How Your Phone Recognizes Your Home: An Investigation Of Mobile Object Recognition</a></h2>
							<h3 style="font-weight: normal">
								<strong>Thanh Vu</strong>,
								 <a href="https://www.linkedin.com/in/danielpiros/">Daniel Piros</a>, 
								<a href="https://scholar.google.com/citations?user=OBuu9_YAAAAJ&hl=en">Amir Sadovnik</a>
								</h3>
								<h3 style="font-weight: normal">
								National Conference on Undergraduate Research (NCUR), 2016
							</h3>
							<!-- <br> -->
							<!-- <p>Often what is effortless for a human brain challenges machines the most. Visual recognition, a fairly easy task for humans, can be surprisingly difficult for machines due to variations in angle, size, and lighting. The challenge is amplified on mobile platforms because of computational constraints. There have been a number of studies on image recognition, but few focus on algorithms that run completely on portable devices. This work presents an improved image retrieval method that can run on mobile devices in real time without the need to access a remote server. First, the speed and accuracy of different known keypoint detectors and descriptors were studied to select the best one. Then, the results were further optimized by filtering best matches, exploiting the user’s location, and extending a grayscale descriptor to include color. The algorithm successfully matched various objects to locally stored sample images with an improved accuracy of 98.5% in less than a second. In addition, since no well-structured database was publicly available, new data sets comprising hundreds of photographs of college buildings and academic posters taken from a combination of distances and angles were built. These data sets will be made publicly available. Finally, an application in the form of an electronic tour guide is presented, where users instantly gain detailed information on buildings or posters by taking pictures of them with their phones or tablets. Although this work focused on images of buildings and posters, the algorithm could potentially be used in other image recognition algorithms.</p> -->
							<p>This work presents an improved image retrieval method that can run completely offline in real time on mobile devices. Our algorithm successfully matched various objects to locally stored sample images with an improved accuracy of 98.5% in less than a second. Although this work focused on images of buildings and posters, the algorithm could potentially be used for other recognition tasks, with possible applications in electronic tour guides.</p>
							<ul class="actions">
								<li><a href="http://www.ncurproceedings.org/ojs/index.php/NCUR2016/article/view/1969" class="button small">pdf</a></li>
								<li><a href="./refs/BuildingRecognition_Vu_2016_NCUR.bib" class="button small">bibtex</a></li>
								<li><a href="https://drive.google.com/open?id=0B4dOg-7qH-2jUUJTSzJjOW1EQkE" class="button small">poster</a></li>
								<li><a href="https://github.com/thanhmvu/OpenCVTour" class="button small">code</a></li>
								<li><a href="https://drive.google.com/open?id=0B4dOg-7qH-2jSWUtR09lM00tTXc" class="button small">data</a></li>
							</ul>
						</t-article-text>
					</div>
					
					<header id="header" class="thanh-horizontal-separator"></header>
					
					<div class="t-posts">
						<t-article-image id="bachmann">
							<a href="https://drive.google.com/file/d/0B2TC6ATRSctxdm8yZk1VdzN3M2M/view?usp=sharing" class="image"><img src="https://lh3.googleusercontent.com/WiSICXPGlchQrCwx_uXmfuSnq3JnZP43M7cSqh7KUgA5fKMLZCnN2m7Rqs18xrls8vlHHuVBmYCOORE9ijTjJKThKHWTzUi0cqat7v_OmMNu7OJUaKpKDYtXrJYLGEMTYS2CPR8nx1PgjjU2e_sVFvMn_5__nsiSNHIBJ7mCP2BgpqURg1FiZyC9fzGjmOmLxU2uuXTTe8mWKXrwOXCKykr_nkkdIQE4eiYdHYmwnmWBzcYxIjl-0Iphmq28FZgZ26PL-HSfge0R76moJBAg3bMgfg0_yaBa24-9rKhajiF9s7INJWIvWGMYr1xbk6-9mrpOGYYySVjMiMVSJOj6RCGUaYDX9bigXVqkY4POy9pRCiw0j-NQPiRmQ1RGHEKUkxBpTKxBjAD4z8CxFZ3mkY6XimJQNIvNPXGKMvCny3iOQHKRWCLoaBuRil73t-yq1T4nQLjCPHgVJzCHz3B9X2TzMhuzN7Rfyb570ZUH8OEQHrV5ENVUi3qQx-lSr4QKaz61VnV4p9cNaushlD8CNS8iaKP6d-idE1RBPxpqhJYhJWRDyrc_B39c5ACqEmCu0cj29xAJ3ecQ_-O6VZrzTWiT-I8N4lUqDm6f-HtbIojNsb1VGcXUgknehP4ho5QBGLM8NNT2R6QIjAbGqTHLVJ3143aUDHpD=w1044-h871-no" alt="" /></a>
						</t-article-image>
						<t-article-text id="bachmann-description">
							<h2><a href="https://drive.google.com/file/d/0B2TC6ATRSctxdm8yZk1VdzN3M2M/view?usp=sharing" style="border-bottom: 0px;">Digital Preservation of German American Heritage Sites in Pennsylvania using Terrestrial Lidar and an Interactive Web-based Interface</a></h2>
							<h3 style="font-weight: normal">
								Christopher Nelsen, 
								Michael Yust,
								<strong>Thanh Vu</strong>,
								<a href="https://scholar.google.com/citations?user=OBuu9_YAAAAJ&hl=en">Amir Sadovnik</a>,
								<a href="https://sites.lafayette.edu/lambfafm/">Margarete Lamb-Faffelberger</a>,
								<a href="https://ce.lafayette.edu/people/michael-p-mcguire/">Michael McGuire</a>
							</h3>
							<h3 style="font-weight: normal">
								Poster presented at the Undergraduate Research Conference in German Studies, 2016
							</h3>
							<!-- <br> -->
							<p>In this work, we preserved the 1753 Bachmann Publick House, Easton's oldest standing building, by LIDAR-scanning it and creating a web interface rendering 1.5 billions data points of a 3D model of the house using Potree, webGL, and Three.js.</p>
							<ul class="actions">
								<li><a href="https://drive.google.com/file/d/0B2TC6ATRSctxdm8yZk1VdzN3M2M/view?usp=sharing" class="button small">poster</a></li>
								<li><a href="https://www.youtube.com/watch?v=SecYi1ALSbU" class="button small">demo</a></li>
							</ul>
						</t-article-text>
					</div>
				</section>

				<header id="header" class="thanh-footer-bar"></header>

				<!-- Employment -->
				<section id="employment">
					<header class="major">
						<h2>EMPLOYMENT</h2>
					</header>
 
 				<h3>
						Research Assistant, <a href="https://cs.unc.edu/">UNC CHAPEL HILL</a>
						<br>
						<span style="font-weight: normal">2018 - Present</span>
					</h3>
					<ul>
						<li>Conducting research under the supervision of <a href="http://frahm.web.unc.edu/">Dr. Jan-Michael Frahm</a></li>
					</ul>
					
					<h3>
						CRLA Certified Academic Tutor, <a href="https://www.lafayette.edu">LAFAYETTE COLLEGE</a>
						<br>
						<span style="font-weight: normal">2015 - 2018</span>
					</h3>
					<ul>
						<li>Led weekly private and group tutoring sessions covering materials in various Computer Science and Mathematics courses</li>
					</ul>

					<h3>
						Software Engineer Intern, <a href="https://www.amazon.com/">AMAZON.COM, INC.</a>
						<br>
						<span style="font-weight: normal">Summer 2017</span>
					</h3>
					<ul>
						<li>Led the development of a Digital Book Store’s project that is now fully launched on Amazon Kindle app (100M+ users)</li>
						<li>Provided users access to more personalized search results while reducing their clicks by 50%</li>
					</ul>

					<h3>
						Research Assistant, <a href="https://www.lafayette.edu">LAFAYETTE COLLEGE</a>
						<br>
						<span style="font-weight: normal">2015 - 2017</span>
					</h3>
					<ul>
						<li>Conducted research under the supervision of <a href="https://scholar.google.com/citations?user=OBuu9_YAAAAJ&hl=en">Dr. Amir Sadovnik</a></li> 
					</ul>

					<h3>
						Teaching Assistant, <a href="https://www.lafayette.edu">LAFAYETTE COLLEGE</a>
						<br>
						<span style="font-weight: normal">2016 - 2017</span>
					</h3>
					<ul>
						<li>TAed CS 150: Data Structures &amp; Algorithms and CS 104: Intro to Game Programming</li>
						<li>Assisted 70+ students during weekly lab sessions</li>
						<li>Proctored written and lab exams</li>
					</ul>
				</section>

				<!-- Contacts -->
				<br/>
				<div class="bar-footer">
					<p class="copyright">
						&copy; 2018 Thanh Vu. All rights reserved. 
						<br>
						Design: <a href="https://html5up.net">HTML5 UP</a>.
					</p>
					<ul class="icons">
						<!-- <li><a href="#" style="font-weight: bold;"><span class="label">CV</span></a></li> -->
						<li><a href="##resume##" class="icon fa-file-text-o" style="color: #468EC9 !important;"><span class="label">Resume</span></a></li>
						<li><a href="mailto:tvu-[at]-cs-[dot]-unc-[dot]-edu" class="icon fa-envelope-o" style="color: #468EC9 !important;"><span class="label">Email</span></a></li>
						<li><a href="https://www.linkedin.com/in/thanhmvu/" class="icon fa-linkedin" style="color: #468EC9 !important;"><span class="label">Linkedin</span></a></li>
						<li><a href="https://github.com/thanhmvu" class="icon fa-github" style="color: #468EC9 !important;"><span class="label">Github</span></a></li>
					</ul>
				</div>

			</div>
		</div>


		<!-- Sidebar -->
		<div id="sidebar">
			<div class="inner">
				<!-- Search -->
				<!-- <section id="search" class="alt">
					<form method="post" action="#">
						<input type="text" name="query" id="query" placeholder="Search" />
					</form>
				</section> -->

				<!-- Menu -->
				<nav id="menu">
					<header class="major">
						<h2>Thanh Vu</h2>
					</header>
					<ul>
						<li><a href="/">About</a></li>
						<li><a href="/#research">Research</a></li>
						<li><a href="/#employment">Employment</a></li>
						<li><a href="/art">Photography</a></li>
						<!-- <li>
							<span class="opener">Visuals</span>
							<ul>
								<li><a href="art">Photography</a></li>
								<li><a href="art#collections">Photo Collections</a></li>
								<li><a href="art#media">Multimedia Art</a></li>
								<li><a href="art#drawing">Drawing</a></li>
							</ul>
						</li> -->
						<li><a href="##resume##">Resume</a></li>
						<!-- <li><a href="dev">Coding</a></li> -->
						<!-- <li><a href="#photography">Photography</a></li> -->
					</ul>
				</nav>

				<!-- Contacts -->
				<section>
					<header class="major">
						<h3>Contacts</h3>
					</header>
					<!-- <p> text here </p> -->
					<ul class="contact">
						<!-- <li class="fa-file-text-o"><a href="https://drive.google.com/open?id=1BLETezyz8qWZRPiUaU5udjkpsuxuCmWq">Resume</a></li> -->
						<li class="fa-envelope-o"><a href="mailto:tvu-[at]-cs-[dot]-unc-[dot]-edu">Email</a></li>
						<li class="fa-linkedin"><a href="https://www.linkedin.com/in/thanhmvu/">LinkedIn</a></li>
						<li class="fa-github"><a href="https://github.com/thanhmvu">Github</a></li>
						<!-- <li class="fa-phone">(xxx) xxx-xxx</li> -->
						<!-- <li class="fa-home"> Chapel Hill, NC 27516</li> -->
					</ul>
				</section>

				<!-- For Fun -->
				<!-- <section>
					<div class="mini-posts">
						<article>
							<p>Now, pick a pill!</p>
							<a href="#" class="image"><img src="images/red_pill_blue_pill.gif" alt="" /></a>
						</article>
					</div>
				</section> -->

				<!-- Footer -->
				<footer id="footer">
					<p class="copyright">&copy; 2018 Thanh Vu. All rights reserved. 
					<br>
					Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
				</footer>
			</div>
		</div>
	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/skel.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>