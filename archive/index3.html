<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="“width=800”">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    heading2 {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 18px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="https://lh3.googleusercontent.com/0NI3S_45B7BNc5XDpKQSTdTjZdD-wjjhJ0XTg-fMy8PXI52lkJEKYwQhEBlDh5RgLC_AvPjVq_li7zPd2zHqX0beZEurbvYdFlpodAuMjHao1rYXzRPG4Rzx7T_9SmsxOVrEC28QE9cXYoYWmaUODs2sMr1ZSKZAPnD-T2zoNNtjxwri0iPHQn70W6jp4CYztbyXn4s9wrqhjLy3TGe-hTe8h-Od999xYeWO1gjtHHpSZz15ToNpi0-ECriNX31NS9Ff0uWXiW3FiLmlo9cLD3rcv97tHmLCJVhqeLOSO6hHwajxH-EEQFNdMmKABr00og6tVVuhvN_GZ3dHXmlMcNitem_PTggXgX4jgMu1vEMDnoKhB0cywUWvlwgnBkV2FRgXW2i2eEBgt9UUve4HO_XJkbYdd9533CQ5GkOGebp2XpWY0iKspLRoQ-z_hCZmZOnSN8wm8mKajD-zCYN6bAzJtg8Hg6IO7bAhrvkCCOJQjlpokH-2M_fxhQ10oGW4dF1Sp7cIpCkneElT-zLB8uTknAlCGmOfgMCTU-o0vHtHzH1Dbk2XaMQD60e9YpJBihx_fD50OQmef3Iw_VOvO0YSBPwvuRr62GLHtcB0GkLcEEe-etlU98neULIJtDwTHueHeWhZymQMaur2nI_8nc9xDSOq3WFu=s100-no"> 
  <title>Thanh M. Vu</title>  
  <link href="./index3_assets/css" rel="stylesheet" type="text/css">
</head>
<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
        <tr>
          <td width="67%" valign="middle">
            <p align="center">
              <name>Thanh M. Vu</name> <br>
              PhD Student, UNC-Chapel Hill
              <!-- tvu [at] cs [dot] unc [dot] edu -->
            </p>
            <p>
              I am an incoming PhD student at the <a href="http://cs.unc.edu/">University of North Carolina at Chapel Hill</a>. My primary research interests are in the areas of computer vision and machine learning.
            </p>
            <p>
              I previously completed my Bachelor's degree in Computer Science with a Minor in Mathematics at <a href="https://www.lafayette.edu/">Lafayette College</a>, Easton, PA. During this time, I worked closely with Dr. <a href="https://compsci.lafayette.edu/people/amir-sadovnik/">Amir Sadovnik</a> and Dr. <a href="https://compsci.lafayette.edu/people/chun-wai-liew/">Chun Wai Liew</a> on multiple research projects.  
            </p>
            <p align="center">
              <a href="mailto:tvu@cs.unc.edu">Email</a> &nbsp;|&nbsp;
              <a href="http://thanhmvu.com/cv">CV</a> &nbsp;|&nbsp;
              <!-- <a href="#">Google Scholar</a> &nbsp;|&nbsp; -->
              <a href="https://www.linkedin.com/in/thanhmvu/">LinkedIn</a> &nbsp;|&nbsp;
              <a href="https://github.com/thanhmvu">Github</a>
            </p>
          </td>
          <td width="33%">
            <img src="https://lh3.googleusercontent.com/34Vc-Zu6pB3u-NdMwaqwRJX1EKFHBqoLkfNIkQ5YlHSFchtwK6DYlK_G587tEzzZNZ-XHMt8c8uuvRkZ8nE4SsDdGs-o--VQPz-IZD0dTug1DHuU1BY1sEow_485cZVq3a7ia32NriZM8nFaHJASVYRUzoyG4tlHKpPQfQ0Ew3UbNW88pGD3JJKMgTRXEXMg41Tfy7DH_nyTahgr9ANz_oJbGTMGxDr-OnwmiEnLHxR8QgrxOtcV4BwZjDJnWnq233WuptBglJa0nnqRBRQJwbfdTgUuMccE302iXYtT8q7st81NaExuDZVCT_uHtH_V4drpmp-0SkAG4O966gDi-79YYk_Nffy6OEU2QTxmi2ytZ7Fdm2_w7QjYrw3YM1tZQpQnC_4kv77sGROvcE3QMTrGPx9omBwmh6ypZMbTz7jUc6EqW54ELCdkHp1Cp-zY4gy4Bs_C4G_FejpvJAr7Q8GJQXUrnzxb9x3NKwT2HNT66-Jz6Vkwwa49J3l3yKID7XEEP4IWkIQVcetNMpXBvAcjxn81zaaEJvvw-CsLeihc22gOB91x1Uu2BSo9zBhoy4jYkYtehOC5Fs2lQMrgR3hR4zEcxjmDecT1DrBO1WuyQC4gQzAPcAgqwUXvkMWlqNS-7MbzOXS500olTbFX1bt9KsyUJTgI=s256-no">
          </td>
        </tr>
        </tbody>
      </table>

			<!-- Research -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	      <tbody>
	      	<tr>
		        <td width="100%" valign="middle">
		          <heading>Research</heading>
		        </td>
	      	</tr>
	      </tbody>
    	</table>
  		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		    <tbody>

          <tr>
            <td width="25%">
              <img src="https://lh3.googleusercontent.com/r17E_d3Gy9zDDRr6mU28mLtbgQYn7glwocgNFdaIvKC-MfCgFZ3qa3tuFJx79Tc-0UVnAfthVuf0X3NpcVOL94gq5WiWeZO7Vg716JzcSn8oe2VF2Wwy9gbW9FZkSLsEBlRBqcZKIc2QP1yk9KzKTa_5cEFTp6gQ2G5CiS2IrSJ_Be7UmXewPRJtOY1SK7VPcl-cWKJKnxG2YUY_f9JMNWsWT9I9ap9AspWvPoQrvQjtcsMzBurBJxtKng924yfjVKjmV9-sWIDqv23UxxFo6WcXjRYSWB60s8x2rMjdRpM9nNIL3vP2WajGobbXYHya4wszDAT9RHvzyr9r3lkxGA0NwGGYI3bcNMgZ-le4iuWtRHm4U9-X6c2Jap9IZr6jkEefKkwF3hXkeqQhwtb4hKs64EbRZyqxmevLqKaXuMKnTsdygLCkHmDT3JJKM3RVOgixHCp5ONe1ozEQ2hODp0P5ck4y9NHnADbaEN9H5S_P1iMfeBbENkan4ZB95XQiI-ENjmsR1Dcuqvgw6i58Vvc8gfkbFMNCrDYCRu1EZ7iRE1pHCCSaNl2dsbn9RSKlnjcgLe08MO6A5taDP2cPDcYux7wWt0-e-t0QP6IuWeyI42Ws6waLQhbEHAw1brtUMs-WkFhgAIkIVw-0--mlx3hXd13Kw4mV=w1023-h629-no" alt="" width="100%">
            </td>
            <td valign="top" width="75%">
              <a href="https://drive.google.com/open?id=1sDmlBC-252EU31fabgctF5ClIA-P-rVo">
                <papertitle>Learning Visual Compatibility: An Improved Method for Visual Compatibility Embedding</papertitle>
              </a>
              <br>
              <strong>Thanh Vu</strong>
              <!-- <a href="https://compsci.lafayette.edu/people/amir-sadovnik/">Amir Sadovnik</a>, -->
              <!-- <a href="https://compsci.lafayette.edu/people/chun-wai-liew/">Chun Wai Liew</a> -->
              <br>
              <em>Honors thesis for B.S. degree</em>, Lafayette College, 2018 
              <br>
              <!-- <a href="https://drive.google.com/open?id=1sDmlBC-252EU31fabgctF5ClIA-P-rVo">paper</a> -->
              <!-- / -->
              <a href="./refs/VisualComp_Vu_2018_Lafayette.bib">bibtex</a>
              <p></p>
              <p>Visual compatibility is a visual property that allows cross-category objects to coordinate well with each other, creating a visually cohesive and appealing collection. This work studies the problem of compatibility learning using deep convolutional neural networks, focusing on the fashion domain. Particularly, it explores a metric learning method to embed images into a compatibility space where more compatible objects are closer to each other. We propose a notion of a visual compatibility hierarchy with three different levels: functional, contextual, and aesthetic compatibility. Based on the levels’ increasing complexities, we propose a training framework that learns the property in a ranking manner and a data-driven method for learning such a hierarchy. Our experimental results demonstrate that the framework outperforms previously proposed methods for embedding visual compatibility. Moreover, our method can learn the property hierarchically and classify aesthetic compatibility from contextual compatibility, which is a much harder task compared to compatible/incompatible classification and is task that has not been addressed directly before.</p> 
            </td>
          </tr>

          <tr>
            <td width="25%">
              <img src="https://lh3.googleusercontent.com/9AYBPT3s96DOWVBkTG6alfmhmk41X73GyugQTgnY6muCzIUK5mSZWLYP8yZj03XZLa5Bt62ASo_xe_X1PiCyOFIlAqIGNzxu_v6ubAEbBY6U-B8Yy2p1rh9htNhYsyoMwdA4vLX6KioFfY95mmXxM6_d4BiXlPfBHXoS89kBLwstnRHXoo9mdyo7pzPpQdz0L37VRJYtMI8xxZkovnLOCw0E8YgFWxa_wUopnhF9heKpBWOPX4-bw3WZFZczcKKXAiR2bYOWpan_QGx8v-a431_qAtVUry3Bz8epAjoIIFOuegwUaw79PodsrXZTJKzTklwRdPoJ1nuz-tI-u1g2pYHQ7mXp7hAAyl_RALgeElO8nCq0dLwgIAYIDpQAIWRdyJTwiSH7KLkASWQPQNmSUvtg2AOvS4hHrGdO5x1N7Ky7C_E_ZqhfdvsxBzCCxTJskOB6HgXvn2T0ybAz4qxXb8TAKyDXdm853aaQYiTtcZGuiZ8GlI6NbeFjtQ536SN3vbYo0zzdLk0JLKhjrucPlho_2MQWeA0N71Ud9GNqw0JF7kTdnjEkGhzAAwifVofduUxNXOVkm0Lf7ai4-df65wwzr8wUuuUa2LAl0sUL8QsRORY6CNGSmTMJK1XUPJ1HnNEKlVPgdOOde37c7veX6Ac51xW_SieB=s1034-no" alt="" width="100%">
            </td>
            <td valign="top" width="75%">
              <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w48/html/Sadovnik_Finding_Your_Lookalike_CVPR_2018_paper.html">
                <papertitle>Finding your Lookalike: Measuring Face Similarity Rather than Face Identity</papertitle>
              </a>
              <br>
              <a href="https://compsci.lafayette.edu/people/amir-sadovnik/">Amir Sadovnik</a>,
              <a href="http://gharbi.me/">Wassim Gharbi</a>,
              <strong>Thanh Vu</strong>,
              <a href="https://ai.google/research/people/AndrewGallagher">Andrew Gallagher</a>
              <br>
              <em>Computer Vision and Pattern Recognition (CVPR) Workshops</em>, 2018 
              <br>
              <!-- <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w48/html/Sadovnik_Finding_Your_Lookalike_CVPR_2018_paper.html">paper</a> -->
              <!-- / -->
              <a href="./refs/FaceSim_Sadovnik_2018_CVPR_Workshops.bib">bibtex</a>
              <p></p>
              <p>Face images are one of the main areas of focus for computer vision, receiving on a wide variety of tasks. Although face recognition is probably the most widely researched, many other tasks such as kinship detection, facial expression classification and facial aging have been examined. In this work we propose the new, subjective task of quantifying perceived face similarity between a pair of faces. That is, we predict the perceived similarity between facial images, given that they are not of the same person. Although this task is clearly correlated with face recognition, it is different and therefore justifies a separate investigation. Humans often remark that two persons look alike, even in cases where the persons are not actually confused with one another. In addition, because face similarity is different than traditional image similarity, there are challenges in data collection and labeling, and dealing with diverging subjective opinions between human labelers. We present evidence that finding facial look-alikes and recognizing faces are two distinct tasks. We propose a new dataset for facial similarity and introduce the Lookalike network, directed towards similar face classification, which outperforms the ad hoc usage of a face recognition network directed at the same task.</p> 
            </td>
          </tr>

          <tr>
            <td width="25%">
              <img src="https://lh3.googleusercontent.com/RkEuZYr3ySf670D-9BokSWSj01iR7IrseMsi7mComWwgu_x9FzHReoBzVIH9MPnGJd3nN6fZ5CZF26RuB_xO8kI3FhUj-3djYNgewxUF-o2plotfEmvVZ4Tnbm8eBCep5m8M83CLuJgQ29Zi1kB0aKMSNPXtYM13kuE9lJc4ay2qYw4McxPWDOgXvYovOVSelrC10NNILLu9_Kj0SizjPRYsOIBWStnAIK13lEjiK6OOeByQ7IJD4OwC3HMoLQhXA9RW6eaRE4Dyih8rGmuXLKv0ocs0W59hCYug-87BvAkqYoWQMZigZrkJma-BfT58geHoK-tkTLtIIIKRPddYrCqK3wMmG2-tgv1BLPwo0lyKk5WKH27kXts4uwc_5LFBpzsetWdEE89GgPCSg3rQJuHw3t90F0SnbQQ9s-NWwN-IgwKGRp8cH-Rth_ZmTd3KnVqoNYXa6RVLbTvKRGI42XjUSG1Qtq0t0ujO-8P1osOPYnwqe1gfM5CvX_91GrI2Vgis9J9FnOhF-XeJtD7r1OQomL9YTdeK_UJHEJ1up8LUE8PiG3Nzp94zPDALG23RJs0rxv2gdQRy19eF4dTd936-_s7UQiTmU94ywBUw1up4fmMUoBXQ1xS5mIfT_C2jEasDwUXzeWpLow5oTnsS0XTj3-Bw0CDF=w1006-h640-no" alt="" width="100%">
            </td>
            <td valign="top" width="75%">
              <a href="https://drive.google.com/file/d/0B2TC6ATRSctxci1uTlFjNU1sR1k">
                <papertitle>Robust Automatic Poster Recognition</papertitle>
              </a>
              <br>
              <strong>Thanh Vu</strong>,
              <a href="https://compsci.lafayette.edu/people/amir-sadovnik/">Amir Sadovnik</a>
              <br>
              <em>Research completed under EXCEL Scholars Program</em>, Lafayette College, 2017 
              <br>
              <!-- <a href="https://drive.google.com/file/d/0B2TC6ATRSctxci1uTlFjNU1sR1k">paper</a> -->
              <!-- / -->
              <a href="./refs/PosterRecognition_Vu_2017_Lafayette.bib">bibtex</a>
              /
              <a href="https://drive.google.com/open?id=0B4dOg-7qH-2jNlhqT0RUbzlpVW8">poster</a>
              /
              <a href="https://github.com/thanhmvu/PosterRecognition">code</a>
              /
              <a href="https://photos.google.com/share/AF1QipNKkFxrgbcIN0NTb1IkVjMQPUx3_OPOXTwHkZesKlinWpJmLTXcLW6e-PLhlZm_yg?key=MS1PbV9WcVFQZ0ZsNmpnLWdWNzNFaFlTamRneDZn">sample data</a>
              <!-- / -->
              <!-- <a href="https://drive.google.com/file/d/0B2TC6ATRSctxZURGQTV2bW5jVGs/view">sample results</a> -->
              <p></p>
              <p>Image recognition has been one of the most well researched tasks in the computer vision field. Many works have been published on recognizing pictures of buildings, works of art and other objects. Traditionally this is done by comparing features extracted from the query image to features extracted from an image database. However, although these produce state of the art results for many types of objects this type of approach does not generalize to all domains. In this paper we address one such domain: academic poster recognition. First we show that because of the unique structure of academic posters and the environment in which they are presented, the traditional approach of feature matching fails. Then we present a new approach for academic poster recognition based on object detection using convolutional networks and show how it outperforms the traditional approaches.</p> 
              <!-- Developed a fast and robust recognition algorithm for occluded, text-heavy academic research posters using a deep convolutional neural network
              Designed a method to automatically generate 2K synthetic training images for the network from a single picture of each poster
              Trained the neural network using the dataset and achieved an accuracy of 92.8% for poster classification, outperforming traditional methods -->
            </td>
          </tr>

          <tr>
            <td width="25%">
              <!-- <img src="https://lh3.googleusercontent.com/pueITIReLLItXn4Z3DEVXbGEOSKrrSzeCX8yOSHd4MvXD3Bd-B5-q80rjojqGvikt7swm1wM64aAOpU-uz1Y0OmqKa7_xUdh03UUw_RBiWucqUuxVHUU9VkF7Eq4swSl846zI13hyjbuulORwFtIBF3cCh74oIe9REe3F9uXBA16ftTJC8efhorWbM0WGg7CVfOpJfdEltVDLJCe-5XNSLpGEB6MMF60M3Ly5I78xUQK4JLsZXMb9P3Nck7oSBH11kuyLc9i8JTzUVVL-0SGsfVRcryCon6FGk4pnmQiTv1t5-UksgSK3o7PYK_WAcYgX3fd_hXyXKJpxCUe4RHD7fdOCNiVZqQPRpE-t_GoGrtVHdQ5AeNiQroHKVw9LKLd8P9JzuOPXNWz3NMEwwKhUfRG0j9PDvhSmeHSdNjLsXQv21zQ3y1m2nhNbnfoWBjNDmPwF29OIKn1SvI94XT9sKoccQMIAiTvsHyi6wLzZ-JrQm2ISFFZ2hoAT0hBtW6UWa1rEoa7iUrGvNd033uWvz8PAwWMFR_Wx5yn9hFpdUGHSLvG67hzcgcP4pQydkDkT51XdX7ldrFOVIjixfb0_vRl7JtXa38C9YP0i-FWUgrchXcc9HCd7ewZRWd2idOAmOurtN-ePEu8CkiRlxGGJLdylVIxiIqkzkz_gMpYGX6fOQ=w800-h500-no" alt="" width="100%"> -->
              <img src="https://lh3.googleusercontent.com/WiSICXPGlchQrCwx_uXmfuSnq3JnZP43M7cSqh7KUgA5fKMLZCnN2m7Rqs18xrls8vlHHuVBmYCOORE9ijTjJKThKHWTzUi0cqat7v_OmMNu7OJUaKpKDYtXrJYLGEMTYS2CPR8nx1PgjjU2e_sVFvMn_5__nsiSNHIBJ7mCP2BgpqURg1FiZyC9fzGjmOmLxU2uuXTTe8mWKXrwOXCKykr_nkkdIQE4eiYdHYmwnmWBzcYxIjl-0Iphmq28FZgZ26PL-HSfge0R76moJBAg3bMgfg0_yaBa24-9rKhajiF9s7INJWIvWGMYr1xbk6-9mrpOGYYySVjMiMVSJOj6RCGUaYDX9bigXVqkY4POy9pRCiw0j-NQPiRmQ1RGHEKUkxBpTKxBjAD4z8CxFZ3mkY6XimJQNIvNPXGKMvCny3iOQHKRWCLoaBuRil73t-yq1T4nQLjCPHgVJzCHz3B9X2TzMhuzN7Rfyb570ZUH8OEQHrV5ENVUi3qQx-lSr4QKaz61VnV4p9cNaushlD8CNS8iaKP6d-idE1RBPxpqhJYhJWRDyrc_B39c5ACqEmCu0cj29xAJ3ecQ_-O6VZrzTWiT-I8N4lUqDm6f-HtbIojNsb1VGcXUgknehP4ho5QBGLM8NNT2R6QIjAbGqTHLVJ3143aUDHpD=w1044-h871-no" alt="" width="100%">
            </td>
            <td valign="top" width="75%">
              <a href="https://drive.google.com/file/d/0B2TC6ATRSctxdm8yZk1VdzN3M2M/view?usp=sharing">
                <papertitle>Digital Preservation of German American Heritage Sites in Pennsylvania using Terrestrial Lidar and an Interactive Web-based Interface</papertitle>
              </a>
              <br>
              Christopher Nelsen, 
              Michael Yust,
              <strong>Thanh Vu</strong>,
              <a href="https://compsci.lafayette.edu/people/amir-sadovnik/">Amir Sadovnik</a>,
              <a href="https://sites.lafayette.edu/lambfafm/">Margarete Lamb-Faffelberger</a>,
              <a href="https://ce.lafayette.edu/people/michael-p-mcguire/">Michael McGuire</a>
              <br>
              <em>Poster presented at the Undergraduate Research Conference in German Studies</em>, 2016 
              <br>
              <a href="https://drive.google.com/file/d/0B2TC6ATRSctxdm8yZk1VdzN3M2M/view?usp=sharing">poster</a>
              /
              <a href ="https://www.youtube.com/watch?v=SecYi1ALSbU">demo</a>
              <p></p>
              <p>We created a 3D model of the 1753 <a href="https://sigalmuseum.org/bachmann/">Bachmann Publick House</a> by LIDAR-scanning and rendering 1.5 billions data points using Potree, webGL, and Three.js</p> 
            </td>
          </tr>

          <tr>
            <td width="25%">
              <img src="https://lh3.googleusercontent.com/S2fsOAivMXCYK1eIM27pD2NMZ2sLbyAz7ZBPIIGLwToPl-28DdsVn3h7u4wGbLOs6T71dyUwnlUstFHFmlqWIuoHfprCrriYIZf5Pop30rnc5NY0EEujxXw__Vd-0IDZQxOKRkfjNtxxVQYKcPLvD498_R5bLKkbqFo6jd0a58Q7mhifI4_gmWx-GiW6f2lna50Fvmo6qcIj-bJANsLnxvzkK4vKLBzUlS93XyLG-xVaHVNopu-sfDU4ms2B0wEEIxQA9nGEHh0ModNNfMHuyKlWaSkwP5Jul6o3jKpegXf8-JeGkwOuF5ApWRjFThrgFY790gdPZeEQN4sgpuYJyCzKQb3XXMQT595_jw_q-nqKWWgz6lqglHRZXHN1zd05IINWRmrCle4L5Yo1vxpRnPfo5phRDn7yDPaNeWFErdcrDavSCsd7fonuUI3WNZWdlYwsefh9GEuYXqiTl2CrZqQX-zfC5nh50VNdM5IMEpYmx-xSTTDr0iy7DDi7ld3Z3kD-rNl6R39wKvsqan-rcE4CK46ldT_5rZJGV5kbFeoOSKKND0_4T-smo3V4o55VaCL2Cjfu-qYQvFuCJp7p935jvW07Y4g3aSsNsnnnDKuejP4X9CRwy3lwZCunbbd7ZSZEThckDxEkWZrnAPt45J2-_rP03i6KkXk8UpXnQcaZ-Q=w800-h500-no" alt="" width="100%">
            </td>
            <td valign="top" width="75%">
              <a href="http://www.ncurproceedings.org/ojs/index.php/NCUR2016/article/view/1969">
                <papertitle>How Your Phone Recognizes Your Home: An Investigation Of Mobile Object Recognition</papertitle>
              </a>
              <br>
              <strong>Thanh Vu</strong>,
              <a href="https://compsci.lafayette.edu/people/amir-sadovnik/">Amir Sadovnik</a>
              <br>
              <em>National Conference on Undergraduate Research (NCUR)</em>, 2016 
              <br>
              <!-- <a href="http://www.ncurproceedings.org/ojs/index.php/NCUR2016/article/view/1969">paper</a> -->
              <!-- / -->
              <a href="./refs/BuildingRecognition_Vu_2016_NCUR.bib">bibtex</a>
              /
              <a href="https://drive.google.com/open?id=0B4dOg-7qH-2jUUJTSzJjOW1EQkE">poster</a>
              /
              <a href ="https://github.com/thanhmvu/OpenCVTour">code</a>
              /
              <a href ="https://drive.google.com/open?id=0B4dOg-7qH-2jSWUtR09lM00tTXc">dataset</a>
              <p></p>
              <p>Often what is effortless for a human brain challenges machines the most. Visual recognition, a fairly easy task for humans, can be surprisingly difficult for machines due to variations in angle, size, and lighting. The challenge is amplified on mobile platforms because of computational constraints. There have been a number of studies on image recognition, but few focus on algorithms that run completely on portable devices. This work presents an improved image retrieval method that can run on mobile devices in real time without the need to access a remote server. First, the speed and accuracy of different known keypoint detectors and descriptors were studied to select the best one. Then, the results were further optimized by filtering best matches, exploiting the user’s location, and extending a grayscale descriptor to include color. The algorithm successfully matched various objects to locally stored sample images with an improved accuracy of 98.5% in less than a second. In addition, since no well-structured database was publicly available, new data sets comprising hundreds of photographs of college buildings and academic posters taken from a combination of distances and angles were built. These data sets will be made publicly available. Finally, an application in the form of an electronic tour guide is presented, where users instantly gain detailed information on buildings or posters by taking pictures of them with their phones or tablets. Although this work focused on images of buildings and posters, the algorithm could potentially be used in other image recognition algorithms.</p> 
            </td>
          </tr>

		    </tbody>
	    </table>
	    
      <!-- Teaching -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
            <heading>Teaching</heading>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" style="padding-left: 20px;">
        <tbody>
          <tr>
            <td>
            <heading2>Lafayette College</heading2>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>
          <tr>
            <td width="25%">
              <img src="https://lh3.googleusercontent.com/-XoPcP2w10cNgfFBK30mv1w1c5wwngU8faezzNWaYMGSVF1RurARZIINTYimf5S3W2QMwKh-YksECl9uMm-2Oo7qtF5UGsKArBRViFYIoAXBuCV6FFFS9EQh44dBXJBe_um8iVlaEeHdB-hGkc3jfu5ps0Sui4bnDakjWrOpnxcJogPIzr-Ja3CCCOhQLziWqbe9LqIl2qHpq0pynjbre50-KpLO1Z_JGchhyg4i-QtJsR4FK6dPebYYGZ2S0Y-q2BzIxCC7sNIb_QJHYWrqU3UVXs4sCF2wT8b2GPU3k7YgAqjqwIuPM3yMuk8uVXei8xi-NclFKEzU8pVKmxF8NswNpciq6gXaxDXxRf5XV0iZdTz-kqChI1ls9engNSCu7R9Tr_udDmh8Rmg32e6huKB6Jak8H5PihRJHBQLzpdqiqIIF-qS8VlJM8A8FDJ-UL6bfe7kpMnuG1314jZPNtXn5jdoobZtQtO7OcB8dZ5ceseHz0MLZoL8W6SLeOjfm7QUKK7NYXSEu19VatC9bPfpPbAACU1mDG39yEajcHhFxOXHe_W1wDk8rDCF4Z3vp4AEb5xaz9eYvYFMMdc0dkkqFyavycZpFwBAzUPiiPDmjKy6LBJghQIMeQJ8fQwl2TN2VBaz74o7C4XuBjpQBMM41MFmRmUzZ=w160-h166-no" alt="Data Structures & Algorithms" width="160" height="160">
            </td>
            <td width="75%" valign="center">
              <p>
                <strong>[TA]</strong> <a href="https://compsci.lafayette.edu/cs150/">CS 150: Data Structures &amp; Algorithms</a> - Spring 2017
              </p>
            </td>
          </tr>
          <tr>
            <td width="25%">
              <img src="https://lh3.googleusercontent.com/AEbRgAwra4DNX57OOGu4ouxbGnk8s0ck04L1reUxCOi9etaLf4cu6usQifwg_YyF65Lc_BMvAnWo1EFf8cOUWmt37XNUJH3Jepoel5iKLoecyPQl7xEehefmm2CuoREUDssdhidoPXxtOHaPJ4JzxUWD-WaivRTIPquwJ8R_ABTzePgiiZJQ42sb5Ld-HsdyOXL6mOpqKU7DQZOwWjDz_OeoRFl1dlOPV8Td4alrVsON9FBuCNIKaIdiwPXYGBdNdDQkd9jTWSgPhbG-Wb5LLkfMuCTIXhSTpUsGEj-mh1XKTAR9ULFUG5Qpnvp0v-_ixW5xHTRJbqJPGAuf_MaFzzLS7MsbZk2dsf_qGnt69qEJPc4GeUZmQDiYbJgORmibggnk_aK06fydlAnPa9oVwdfaCm02LoY6Uwqi94MSYaB5OhjOImGNB3-179uofo4yfNGjleLWXTa65XBjuOwkHGQ8kBnkyWNseaGqT7fBAjrEOWnLBmpvtrZRhfFmel65o0fQOZEJz_cIU9FFBRSRL22nVkLtEtS7b2pFRshaSzX5nCDz1SBySHOOx5OhhdGWaCh86zfK5gVrqsU8UH40NlOUcM250zlkIPsHNnEMmHtKdqnQ3jFUKzGcae5vCQ2ESF747J2JwjpUvR2gQbQOBHxAXFwpDhah=s160-no" alt="Game Programming" width="160" height="160">
            </td>
            <td width="75%" valign="center">
              <p>
                <strong>[TA]</strong> <a href="https://compsci.lafayette.edu/cs104">CS 104: Introduction to Game Programming</a> - Spring 2016
              </p>
            </td>
          </tr>
        </tbody>
      </table>

      <!-- Footer -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                  <a href="https://jonbarron.info/">Simple &amp; Nice</a>
        	      </font>
              </p>
            </td>
          </tr>
        </tbody>
      </table>
      <!-- <script type="text/javascript">
        var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
      </script>
      <script src="./index3_assets/ga.js" type="text/javascript"></script>
      <script type="text/javascript">
        try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
            } catch(err) {}
      </script> -->
    </td>
    </tr>
    </tbody>
  </table>
</body>
</html>